{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run ../src/notebook_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(data):\n",
    "    loss_train_list = data['loss_train']\n",
    "    loss_test_list = data['loss_val']\n",
    "    acc_test_list = data['acc_val']\n",
    "    epochs = len(loss_train_list)\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.plot(range(len(loss_train_list)),loss_train_list)\n",
    "    ax.plot(range(len(loss_test_list)),loss_test_list)\n",
    "    plt.title('Loss')\n",
    "    \n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.plot(range(len(acc_test_list)),acc_test_list)\n",
    "    plt.title('Test accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dataloader(data,loader_name,encoding_dict):\n",
    "    if loader_name == 'train_data':\n",
    "        print('Train')\n",
    "    elif loader_name == 'val_data':\n",
    "        print('Validation')\n",
    "        \n",
    "    loader = data[loader_name]\n",
    "    y_encoded = loader.dataset.y\n",
    "    size_train = len(y_encoded)\n",
    "    \n",
    "    data_dict = {encoding_dict[k]:v for k,v in get_imgs_per_cat(y_encoded).items()}\n",
    "    print(f'number of images: {size_train}')\n",
    "    print('Percentage of images in each category:\\n') \n",
    "    for k,v in data_dict.items():\n",
    "        print('{}: {:.6g} %'.format(k,100.0*v/size_train)) \n",
    "        \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do: include early stopping\n",
    "# padding the vectors to the maximum number of epochs\n",
    "\n",
    "#read simple split\n",
    "def read_info_split(split_path,plot = False,show=False):\n",
    "\n",
    "    training_info_path = os.path.join(split_path,'training_info.pth')\n",
    "    try:\n",
    "        training_info = torch.load(training_info_path)\n",
    "    except:\n",
    "        training_info = {}\n",
    "        \n",
    "    output_dict = {}\n",
    "        \n",
    "    if training_info:\n",
    "        \n",
    "        try:\n",
    "            encoding_dict = training_info['encoding_dict']\n",
    "        except:\n",
    "            encoding_dict = None\n",
    "            \n",
    "        \n",
    "        #print(training_info.keys())\n",
    "        #model_name = training_info['model_name']\n",
    "        \n",
    "        #training history\n",
    "        #loss_train = np.array(training_info['loss_train'])\n",
    "        loss_val = np.array(training_info['loss_val'])\n",
    "        acc_val = np.array(training_info['acc_val'])\n",
    "        f1_val = np.array(training_info['f1_val'])\n",
    "        precision_val = np.array(training_info['precision_val'])\n",
    "        recall_val = np.array(training_info['recall_val'])\n",
    "        sensitivity_val = np.array(training_info['sensitivity_val'])\n",
    "        specificity_val = np.array(training_info['specificity_val'])\n",
    "        \n",
    "        \n",
    "        #epochs = training_info['epochs']\n",
    "        \n",
    "        loss_test = training_info['loss_test']\n",
    "        acc_test = training_info['acc_test']\n",
    "        f1_test = training_info['f1_test']\n",
    "        precision_test = training_info['precision_test']\n",
    "        recall_test = training_info['recall_test']\n",
    "        sensitivity_test = np.array(training_info['sensitivity_test'])\n",
    "        specificity_test = np.array(training_info['specificity_test'])\n",
    "        confusion_matrix_test = training_info['confusion_matrix_test']\n",
    "                \n",
    "        \n",
    "        #print(epochs)\n",
    "\n",
    "        i = np.argmin(loss_val)\n",
    "        min_loss = loss_val[i]\n",
    "        #encoding_dict = training_info['encoding_dict']\n",
    "        \n",
    "        if show:\n",
    "            #print(f'model_name: {model_name}')\n",
    "            print(f'min loss: {min_loss} at epoch {i}')\n",
    "            \n",
    "            print(f'acc_test: {loss_test}')\n",
    "            print(f'acc_test: {acc_test}')\n",
    "            print(f'confusion matrix test: ')\n",
    "            print(confusion_matrix_test)\n",
    "            #print(f'encoding_dict = {encoding_dict}')\n",
    "            #show_dataloader(training_info,'test_data',encoding_dict)\n",
    "        \n",
    "        output_dict.update({'encoding_dict':encoding_dict,\n",
    "                            'loss_val':loss_val,'acc_val':acc_val,'f1_val':f1_val,\n",
    "                            'precision_val':precision_val,'recall_val':recall_val,\n",
    "                            'sensitivity_val':sensitivity_val,'specificity_val':specificity_val,\n",
    "                            'loss_test':loss_test,'acc_test':acc_test,'f1_test':f1_test,\n",
    "                            'precision_test':precision_test,'recall_test':recall_test,\n",
    "                            'sensitivity_test':sensitivity_test,'specificity_test':specificity_test,\n",
    "                            'confusion_matrix_test':confusion_matrix_test})\n",
    "        if plot:\n",
    "            plot_training_history(training_info)\n",
    "            \n",
    "    return output_dict\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#split_path = '/projects/sortifier/experimental/nilt_crossvalidation_lr_0,001/split_1'\n",
    "#read_info_split(split_path,show=True,plot = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'loss_val' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9f5c1e0873ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0mresults_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/jcejudo/rd-img-classification-pilot/results'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mcm_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'confusion_matrix_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9f5c1e0873ac>\u001b[0m in \u001b[0;36mread_training_history\u001b[0;34m(results_path)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     return {'encoding_dict':encoding_dict,'loss_val':loss_val,'acc_val':acc_val,'f1_val':f1_val,'precision_val':precision_val,'recall_val':recall_val,\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0;34m'sensitivity_val'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msensitivity_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'specificity_val'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mspecificity_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;34m'acc_test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'f1_test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mf1_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'precision_test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mprecision_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recall_test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrecall_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'loss_val' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#training crossvalidation\n",
    "def read_training_history(results_path):\n",
    "    split_list = os.listdir(results_path)\n",
    "    confusion_matrix_test = []\n",
    "    for i,split in enumerate(split_list):\n",
    "        encoding_dict = None\n",
    "        split_path = os.path.join(results_path,split)\n",
    "        _metrics_dict = read_info_split(split_path)\n",
    "        if _metrics_dict:\n",
    "            #epochs = _metrics_dict['epochs']\n",
    "            epochs = 60\n",
    "            try:\n",
    "                encoding_dict = _metrics_dict['encoding_dict']\n",
    "            except:\n",
    "                encoding_dict = None\n",
    "            \n",
    "            confusion_matrix_test.append(_metrics_dict['confusion_matrix_test'])\n",
    "            \n",
    "            if i == 0:\n",
    "                #loss_train = _metrics_dict['loss_train']\n",
    "                loss_val = _metrics_dict['loss_val']\n",
    "                acc_val = _metrics_dict['acc_val']\n",
    "                f1_val = _metrics_dict['f1_val']\n",
    "                precision_val = _metrics_dict['precision_val']\n",
    "                recall_val = _metrics_dict['recall_val']\n",
    "                sensitivity_val = _metrics_dict['sensitivity_val']\n",
    "                specificity_val = _metrics_dict['specificity_val']\n",
    "                \n",
    "                \n",
    "                                \n",
    "                #loss_train = np.pad(loss_train,(0,epochs-loss_train.shape[0]),'minimum')\n",
    "                loss_val = np.pad(loss_val,(0,epochs-loss_val.shape[0]),'minimum')\n",
    "                acc_val = np.pad(acc_val,(0,epochs-acc_val.shape[0]),'maximum')\n",
    "                f1_val = np.pad(f1_val,(0,epochs-f1_val.shape[0]),'maximum')\n",
    "                precision_val = np.pad(precision_val,(0,epochs-precision_val.shape[0]),'maximum')\n",
    "                recall_val = np.pad(recall_val,(0,epochs-recall_val.shape[0]),'maximum')\n",
    "                sensitivity_val = np.pad(sensitivity_val,(0,epochs-sensitivity_val.shape[0]),'maximum')\n",
    "                specificity_val = np.pad(specificity_val,(0,epochs-specificity_val.shape[0]),'maximum')\n",
    "                           \n",
    "                acc_test = np.array(_metrics_dict['acc_test'])\n",
    "                f1_test = np.array(_metrics_dict['f1_test'])\n",
    "                precision_test = np.array(_metrics_dict['precision_test'])\n",
    "                recall_test = np.array(_metrics_dict['recall_test'])\n",
    "                sensitivity_test = np.array(_metrics_dict['sensitivity_test'])\n",
    "                specificity_test = np.array(_metrics_dict['specificity_test'])\n",
    "                loss_test = np.array(_metrics_dict['loss_test'])\n",
    "                #confusion_matrix_test = np.array(_metrics_dict['confusion_matrix_test'])\n",
    "                 \n",
    "                \n",
    "            else:\n",
    "                \n",
    "                #_loss_train = _metrics_dict['loss_train']\n",
    "                _loss_val = _metrics_dict['loss_val']\n",
    "                _acc_val = _metrics_dict['acc_val']\n",
    "                _f1_val = _metrics_dict['f1_val']\n",
    "                _precision_val = _metrics_dict['precision_val']\n",
    "                _recall_val = _metrics_dict['recall_val']\n",
    "                _sensitivity_val = _metrics_dict['sensitivity_val']\n",
    "                _specificity_val = _metrics_dict['specificity_val']\n",
    "                \n",
    "                # _loss_train = np.pad(_loss_train,(0,epochs-_loss_train.shape[0]),'minimum')\n",
    "                _loss_val = np.pad(_loss_val,(0,epochs-_loss_val.shape[0]),'minimum')\n",
    "                _acc_val = np.pad(_acc_val,(0,epochs-_acc_val.shape[0]),'maximum')\n",
    "                _f1_val = np.pad(_f1_val,(0,epochs-_f1_val.shape[0]),'maximum')\n",
    "                _precision_val = np.pad(_precision_val,(0,epochs-_precision_val.shape[0]),'maximum')\n",
    "                _recall_val = np.pad(_recall_val,(0,epochs-_recall_val.shape[0]),'maximum')\n",
    "                _sensitivity_val = np.pad(_sensitivity_val,(0,epochs-_sensitivity_val.shape[0]),'maximum')\n",
    "                _specificity_val = np.pad(_specificity_val,(0,epochs-_specificity_val.shape[0]),'maximum')\n",
    "                           \n",
    "                \n",
    "                    \n",
    "                #loss_train = np.vstack([loss_train, _loss_train])\n",
    "                loss_val = np.vstack([loss_val, _loss_val])\n",
    "                acc_val = np.vstack([acc_val, _acc_val])\n",
    "                f1_val = np.vstack([f1_val, _f1_val])\n",
    "                precision_val = np.vstack([precision_val, _precision_val])\n",
    "                recall_val = np.vstack([recall_val, _recall_val])\n",
    "                sensitivity_val = np.vstack([sensitivity_val, _sensitivity_val])\n",
    "                specificity_val = np.vstack([specificity_val, _specificity_val])\n",
    "                \n",
    "                \n",
    "                _acc_test = np.array(_metrics_dict['acc_test'])\n",
    "                _f1_test = np.array(_metrics_dict['f1_test'])\n",
    "                _precision_test = np.array(_metrics_dict['precision_test'])\n",
    "                _recall_test = np.array(_metrics_dict['recall_test'])\n",
    "                _sensitivity_test = np.array(_metrics_dict['sensitivity_test'])\n",
    "                _specificity_test = np.array(_metrics_dict['specificity_test'])\n",
    "                _loss_test = np.array(_metrics_dict['loss_test'])\n",
    "                #_confusion_matrix_test = np.array(_metrics_dict['confusion_matrix_test'])\n",
    "                \n",
    "                acc_test = np.vstack([acc_test, _acc_test])\n",
    "                f1_test = np.vstack([f1_test, _f1_test])\n",
    "                precision_test = np.vstack([precision_test, _precision_test])\n",
    "                recall_test = np.vstack([recall_test, _recall_test])\n",
    "                sensitivity_test = np.vstack([sensitivity_test, _sensitivity_test])\n",
    "                specificity_test = np.vstack([specificity_test, _specificity_test])\n",
    "                loss_test = np.vstack([loss_test, _loss_test])\n",
    "                #confusion_matrix_test = np.concatenate((confusion_matrix_test, _confusion_matrix_test),axis=0)\n",
    "                \n",
    "    \n",
    "    return {'encoding_dict':encoding_dict,'loss_val':loss_val,'acc_val':acc_val,'f1_val':f1_val,'precision_val':precision_val,'recall_val':recall_val,\n",
    "            'sensitivity_val':sensitivity_val,'specificity_val':specificity_val,\n",
    "            'acc_test':acc_test,'f1_test':f1_test, 'precision_test':precision_test, 'recall_test':recall_test,\n",
    "            'sensitivity_test':sensitivity_test,'specificity_test':specificity_test,\n",
    "            'loss_test':loss_test,'confusion_matrix_test':confusion_matrix_test}\n",
    "\n",
    "def mean_std_metric(metric):\n",
    "    mean = np.mean(metric,axis=0)\n",
    "    std = np.std(metric,axis=0)\n",
    "    return mean, std\n",
    "\n",
    "def plot_mean_std(metrics_dict,title,fontsize=20):\n",
    "    \n",
    "    fig,ax = plt.subplots()\n",
    "    for k,v in metrics_dict.items():\n",
    "        mean, std = mean_std_metric(v)\n",
    "        ax.plot(mean,label = k)\n",
    "        ax.fill_between(range(mean.shape[0]), mean-std, mean+std, alpha = 0.5)\n",
    "        ax.set_title(title,fontsize=fontsize)\n",
    "        ax.set_ylim((0.94,1.0))\n",
    "        ax.set_xlabel('epochs',fontsize=fontsize)\n",
    "        plt.xticks(fontsize=fontsize)\n",
    "        plt.yticks(fontsize=fontsize)\n",
    "        plt.legend(fontsize=fontsize)\n",
    "        ax.grid()\n",
    "        \n",
    "        \n",
    "\n",
    "results_path = '/home/jcejudo/rd-img-classification-pilot/results'\n",
    "\n",
    "output_dict = read_training_history(results_path)   \n",
    "\n",
    "cm_list = output_dict['confusion_matrix_test']\n",
    "\n",
    "for cm in cm_list:\n",
    "    print(cm)\n",
    "\n",
    "acc_test_mean, acc_test_std = mean_std_metric(output_dict['acc_test'])\n",
    "f1_test_mean, f1_test_std = mean_std_metric(output_dict['f1_test'])\n",
    "\n",
    "\n",
    "print(f'acc_test_mean: {acc_test_mean[0]:.3f}')\n",
    "print(f'acc_test_std: {acc_test_std[0]:.3f}')\n",
    "\n",
    "print(f'f1_test_mean: {f1_test_mean[0]:.3f}')\n",
    "print(f'f1_test_std: {f1_test_std[0]:.3f}')\n",
    "\n",
    "#plot_mean_std({'loss_train':output_dict['loss_train'],'loss_val':output_dict['loss_val']})\n",
    "plot_mean_std({'acc_val_resnet':output_dict['acc_val']},'validation accuracy')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet\n",
    "\n",
    "\n",
    "fontsize = 30\n",
    "results_path = '/projects/sortifier/paper_no_cbct/new_metrics_resnet34_28_10_2020'\n",
    "\n",
    "output_dict_resnet = read_training_history(results_path)  \n",
    "acc_test_mean, acc_test_std = mean_std_metric(output_dict_resnet['acc_test'])\n",
    "f1_test_mean, f1_test_std = mean_std_metric(output_dict_resnet['f1_test'])\n",
    "precision_test_mean, precision_test_std = mean_std_metric(output_dict_resnet['precision_test'])\n",
    "recall_test_mean, recall_test_std = mean_std_metric(output_dict_resnet['recall_test'])\n",
    "sensitivity_test_mean, sensitivity_test_std = mean_std_metric(output_dict_resnet['sensitivity_test'])\n",
    "specificity_test_mean, specificity_test_std = mean_std_metric(output_dict_resnet['specificity_test'])\n",
    "\n",
    "if output_dict_resnet['encoding_dict']:\n",
    "    print(output_dict_resnet['encoding_dict'])\n",
    "\n",
    "print('resnet')\n",
    "print(f'accuracy: {acc_test_mean[0]:.3f} +- {acc_test_std[0]:.3f}')\n",
    "print(f'f1: {f1_test_mean[0]:.3f} +- {f1_test_std[0]:.3f}')\n",
    "print(f'precision: {precision_test_mean[0]:.3f} +- {precision_test_std[0]:.3f}')\n",
    "print(f'sensitivity: {sensitivity_test_mean[0]:.3f} +- {sensitivity_test_std[0]:.3f}')\n",
    "print(f'specificity: {specificity_test_mean[0]:.3f} +- {specificity_test_std[0]:.3f}')\n",
    "\n",
    "plot_mean_std({'accuracy':output_dict_resnet['acc_val'],\n",
    "               'f1':output_dict_resnet['f1_val'],\n",
    "               'precision':output_dict_resnet['precision_val'],\n",
    "               'sensitivity':output_dict_resnet['sensitivity_val'],\n",
    "               'specificity':output_dict_resnet['specificity_val'],\n",
    "              },'ResNet',fontsize=fontsize)\n",
    "\n",
    "\n",
    "cm_list = output_dict_resnet['confusion_matrix_test']\n",
    "for i,cm in enumerate(cm_list):\n",
    "    print(f'split {i}')\n",
    "    print(cm)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "print(50*'-')   \n",
    "    \n",
    "    \n",
    "#CNN\n",
    "\n",
    "results_path = '/projects/sortifier/paper_no_cbct/new_metrics_CNN_28_10_2020'\n",
    "\n",
    "output_dict_resnet = read_training_history(results_path)  \n",
    "acc_test_mean, acc_test_std = mean_std_metric(output_dict_resnet['acc_test'])\n",
    "f1_test_mean, f1_test_std = mean_std_metric(output_dict_resnet['f1_test'])\n",
    "precision_test_mean, precision_test_std = mean_std_metric(output_dict_resnet['precision_test'])\n",
    "recall_test_mean, recall_test_std = mean_std_metric(output_dict_resnet['recall_test'])\n",
    "sensitivity_test_mean, sensitivity_test_std = mean_std_metric(output_dict_resnet['sensitivity_test'])\n",
    "specificity_test_mean, specificity_test_std = mean_std_metric(output_dict_resnet['specificity_test'])\n",
    "\n",
    "if output_dict_resnet['encoding_dict']:\n",
    "    print(output_dict_resnet['encoding_dict'])\n",
    "\n",
    "print('CNN')\n",
    "print(f'accuracy: {acc_test_mean[0]:.3f} +- {acc_test_std[0]:.3f}')\n",
    "print(f'f1: {f1_test_mean[0]:.3f} +- {f1_test_std[0]:.3f}')\n",
    "print(f'precision: {precision_test_mean[0]:.3f} +- {precision_test_std[0]:.3f}')\n",
    "print(f'sensitivity: {sensitivity_test_mean[0]:.3f} +- {sensitivity_test_std[0]:.3f}')\n",
    "print(f'specificity: {specificity_test_mean[0]:.3f} +- {specificity_test_std[0]:.3f}')\n",
    "\n",
    "plot_mean_std({'accuracy':output_dict_resnet['acc_val'],\n",
    "               'f1':output_dict_resnet['f1_val'],\n",
    "               'precision':output_dict_resnet['precision_val'],\n",
    "               'sensitivity':output_dict_resnet['sensitivity_val'],\n",
    "               'specificity':output_dict_resnet['specificity_val'],\n",
    "              },'Baseline',fontsize=fontsize)\n",
    "\n",
    "\n",
    "cm_list = output_dict_resnet['confusion_matrix_test']\n",
    "for i,cm in enumerate(cm_list):\n",
    "    print(f'split {i}')\n",
    "    print(cm)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "#capsules\n",
    "\n",
    "results_path = '/projects/sortifier/paper_no_cbct/new_metrics_capsnet_28_10_2020'\n",
    "\n",
    "output_dict_resnet = read_training_history(results_path)  \n",
    "acc_test_mean, acc_test_std = mean_std_metric(output_dict_resnet['acc_test'])\n",
    "f1_test_mean, f1_test_std = mean_std_metric(output_dict_resnet['f1_test'])\n",
    "precision_test_mean, precision_test_std = mean_std_metric(output_dict_resnet['precision_test'])\n",
    "recall_test_mean, recall_test_std = mean_std_metric(output_dict_resnet['recall_test'])\n",
    "sensitivity_test_mean, sensitivity_test_std = mean_std_metric(output_dict_resnet['sensitivity_test'])\n",
    "specificity_test_mean, specificity_test_std = mean_std_metric(output_dict_resnet['specificity_test'])\n",
    "\n",
    "if output_dict_resnet['encoding_dict']:\n",
    "    print(output_dict_resnet['encoding_dict'])\n",
    "\n",
    "print('capsules')\n",
    "print(f'accuracy: {acc_test_mean[0]:.3f} +- {acc_test_std[0]:.3f}')\n",
    "print(f'f1: {f1_test_mean[0]:.3f} +- {f1_test_std[0]:.3f}')\n",
    "print(f'precision: {precision_test_mean[0]:.3f} +- {precision_test_std[0]:.3f}')\n",
    "print(f'sensitivity: {sensitivity_test_mean[0]:.3f} +- {sensitivity_test_std[0]:.3f}')\n",
    "print(f'specificity: {specificity_test_mean[0]:.3f} +- {specificity_test_std[0]:.3f}')\n",
    "\n",
    "plot_mean_std({'accuracy':output_dict_resnet['acc_val'],\n",
    "               'f1':output_dict_resnet['f1_val'],\n",
    "               'precision':output_dict_resnet['precision_val'],\n",
    "               'sensitivity':output_dict_resnet['sensitivity_val'],\n",
    "               'specificity':output_dict_resnet['specificity_val'],\n",
    "              },'CapsNet',fontsize=fontsize)\n",
    "\n",
    "\n",
    "cm_list = output_dict_resnet['confusion_matrix_test']\n",
    "for i,cm in enumerate(cm_list):\n",
    "    print(f'split {i}')\n",
    "    print(cm)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_conf_matrix(array,columns,font_scale=3):\n",
    "\n",
    "    df_cm = pd.DataFrame(array, index = columns,\n",
    "                      columns = columns)\n",
    "\n",
    "    sn.set(font_scale=font_scale)  # crazy big\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True,cmap=\"YlGnBu\",fmt=\"d\")\n",
    "    plt.xlabel('predicted')\n",
    "    plt.ylabel('ground truth')\n",
    "\n",
    "#resnet\n",
    "array = [[ 736,    0 ,   0 ,   2],\n",
    " [   7  ,133,    0  ,  0],\n",
    " [   0 ,   0, 1512  ,  2],\n",
    " [  10  ,  1  ,  1,  892]]\n",
    "\n",
    "#baseline\n",
    "\n",
    "array = [[ 734,    0 ,   0 ,   1],\n",
    " [   7,  132 ,   0  ,  0],\n",
    " [   1  ,  1, 1510  ,  3],\n",
    " [  69  ,  1 ,   3 , 834]]\n",
    "\n",
    "    \n",
    "#capsules   \n",
    "array = [[ 734 ,   0 ,   0   , 3],\n",
    " [   5 , 135  ,  0  ,  0],\n",
    " [   0  ,  0, 1516  ,  0],\n",
    " [  79  ,  0  ,  0 , 824]]\n",
    "\n",
    "columns = ['bite','ceph', 'pan','per']    \n",
    "plot_conf_matrix(array,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = '/projects/sortifier/bite_pan_other_results/new_metrics_resnet34_28_10_2020'\n",
    "\n",
    "output_dict_resnet = read_training_history(results_path)  \n",
    "acc_test_mean, acc_test_std = mean_std_metric(output_dict_resnet['acc_test'])\n",
    "f1_test_mean, f1_test_std = mean_std_metric(output_dict_resnet['f1_test'])\n",
    "precision_test_mean, precision_test_std = mean_std_metric(output_dict_resnet['precision_test'])\n",
    "recall_test_mean, recall_test_std = mean_std_metric(output_dict_resnet['recall_test'])\n",
    "sensitivity_test_mean, sensitivity_test_std = mean_std_metric(output_dict_resnet['sensitivity_test'])\n",
    "specificity_test_mean, specificity_test_std = mean_std_metric(output_dict_resnet['specificity_test'])\n",
    "\n",
    "\n",
    "\n",
    "print('resnet')\n",
    "print(f'accuracy: {acc_test_mean[0]:.3f} +- {acc_test_std[0]:.3f}')\n",
    "print(f'f1: {f1_test_mean[0]:.3f} +- {f1_test_std[0]:.3f}')\n",
    "print(f'precision: {precision_test_mean[0]:.3f} +- {precision_test_std[0]:.3f}')\n",
    "print(f'recall: {recall_test_mean[0]:.3f} +- {recall_test_std[0]:.3f}')\n",
    "print(f'sensitivity: {sensitivity_test_mean[0]:.3f} +- {sensitivity_test_std[0]:.3f}')\n",
    "print(f'specificity: {specificity_test_mean[0]:.3f} +- {specificity_test_std[0]:.3f}')\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "results_path = '/projects/sortifier/paper_no_cbct/new_metrics_CNN_28_10_2020'\n",
    "\n",
    "output_dict_CNN = read_training_history(results_path)  \n",
    "acc_test_mean, acc_test_std = mean_std_metric(output_dict_CNN['acc_test'])\n",
    "f1_test_mean, f1_test_std = mean_std_metric(output_dict_CNN['f1_test'])\n",
    "precision_test_mean, precision_test_std = mean_std_metric(output_dict_CNN['precision_test'])\n",
    "recall_test_mean, recall_test_std = mean_std_metric(output_dict_CNN['recall_test'])\n",
    "sensitivity_test_mean, sensitivity_test_std = mean_std_metric(output_dict_CNN['sensitivity_test'])\n",
    "specificity_test_mean, specificity_test_std = mean_std_metric(output_dict_CNN['specificity_test'])\n",
    "\n",
    "\n",
    "print('CNN')\n",
    "print(f'accuracy: {acc_test_mean[0]:.3f} +- {acc_test_std[0]:.3f}')\n",
    "print(f'f1: {f1_test_mean[0]:.3f} +- {f1_test_std[0]:.3f}')\n",
    "print(f'precision: {precision_test_mean[0]:.3f} +- {precision_test_std[0]:.3f}')\n",
    "print(f'recall: {recall_test_mean[0]:.3f} +- {recall_test_std[0]:.3f}')\n",
    "print(f'sensitivity: {sensitivity_test_mean[0]:.3f} +- {sensitivity_test_std[0]:.3f}')\n",
    "print(f'specificity: {specificity_test_mean[0]:.3f} +- {specificity_test_std[0]:.3f}')\n",
    "print('\\n')\n",
    "\n",
    "results_path = '/projects/sortifier/paper_no_cbct/new_metrics_capsnet_28_10_2020'\n",
    "\n",
    "output_dict_capsules = read_training_history(results_path)  \n",
    "acc_test_mean, acc_test_std = mean_std_metric(output_dict_capsules['acc_test'])\n",
    "f1_test_mean, f1_test_std = mean_std_metric(output_dict_capsules['f1_test'])\n",
    "precision_test_mean, precision_test_std = mean_std_metric(output_dict_capsules['precision_test'])\n",
    "recall_test_mean, recall_test_std = mean_std_metric(output_dict_capsules['recall_test'])\n",
    "sensitivity_test_mean, sensitivity_test_std = mean_std_metric(output_dict_capsules['sensitivity_test'])\n",
    "specificity_test_mean, specificity_test_std = mean_std_metric(output_dict_capsules['specificity_test'])\n",
    "\n",
    "\n",
    "print('capsules')\n",
    "print(f'accuracy: {acc_test_mean[0]:.3f} +- {acc_test_std[0]:.3f}')\n",
    "print(f'f1: {f1_test_mean[0]:.3f} +- {f1_test_std[0]:.3f}')\n",
    "print(f'precision: {precision_test_mean[0]:.3f} +- {precision_test_std[0]:.3f}')\n",
    "print(f'recall: {recall_test_mean[0]:.3f} +- {recall_test_std[0]:.3f}')\n",
    "print(f'sensitivity: {sensitivity_test_mean[0]:.3f} +- {sensitivity_test_std[0]:.3f}')\n",
    "print(f'specificity: {specificity_test_mean[0]:.3f} +- {specificity_test_std[0]:.3f}')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "plot_mean_std({'CNN':output_dict_CNN['acc_val'],\n",
    "               'resnet':output_dict_resnet['acc_val'],\n",
    "              'capsules':output_dict_capsules['acc_val']},'validation accuracy') \n",
    "\n",
    "plot_mean_std({'CNN':output_dict_CNN['f1_val'],\n",
    "               'resnet':output_dict_resnet['f1_val'],\n",
    "              'capsules':output_dict_capsules['f1_val']},'validation f1') \n",
    "\n",
    "plot_mean_std({'CNN':output_dict_CNN['precision_val'],\n",
    "               'resnet':output_dict_resnet['precision_val'],\n",
    "              'capsules':output_dict_capsules['precision_val']},'validation precision') \n",
    "\n",
    "plot_mean_std({'CNN':output_dict_CNN['recall_val'],\n",
    "               'resnet':output_dict_resnet['recall_val'],\n",
    "              'capsules':output_dict_capsules['recall_val']},'validation recall') \n",
    "\n",
    "plot_mean_std({'CNN':output_dict_CNN['sensitivity_val'],\n",
    "               'resnet':output_dict_resnet['sensitivity_val'],\n",
    "              'capsules':output_dict_capsules['sensitivity_val']},'validation sensitivity') \n",
    "\n",
    "plot_mean_std({'CNN':output_dict_CNN['specificity_val'],\n",
    "               'resnet':output_dict_resnet['specificity_val'],\n",
    "              'capsules':output_dict_capsules['specificity_val']},'validation specificity') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single model\n",
    "\n",
    "results_path = '/projects/sortifier/bite_pan_other_results/new_metrics_resnet34_28_10_2020'\n",
    "\n",
    "output_dict_resnet = read_training_history(results_path)  \n",
    "acc_test_mean, acc_test_std = mean_std_metric(output_dict_resnet['acc_test'])\n",
    "f1_test_mean, f1_test_std = mean_std_metric(output_dict_resnet['f1_test'])\n",
    "precision_test_mean, precision_test_std = mean_std_metric(output_dict_resnet['precision_test'])\n",
    "recall_test_mean, recall_test_std = mean_std_metric(output_dict_resnet['recall_test'])\n",
    "sensitivity_test_mean, sensitivity_test_std = mean_std_metric(output_dict_resnet['sensitivity_test'])\n",
    "specificity_test_mean, specificity_test_std = mean_std_metric(output_dict_resnet['specificity_test'])\n",
    "\n",
    "if output_dict_resnet['encoding_dict']:\n",
    "    print(output_dict_resnet['encoding_dict'])\n",
    "\n",
    "print('resnet')\n",
    "print(f'accuracy: {acc_test_mean[0]:.3f} +- {acc_test_std[0]:.3f}')\n",
    "print(f'f1: {f1_test_mean[0]:.3f} +- {f1_test_std[0]:.3f}')\n",
    "print(f'precision: {precision_test_mean[0]:.3f} +- {precision_test_std[0]:.3f}')\n",
    "print(f'sensitivity: {sensitivity_test_mean[0]:.3f} +- {sensitivity_test_std[0]:.3f}')\n",
    "print(f'specificity: {specificity_test_mean[0]:.3f} +- {specificity_test_std[0]:.3f}')\n",
    "\n",
    "plot_mean_std({'accuracy':output_dict_resnet['acc_val'],\n",
    "               'f1':output_dict_resnet['f1_val'],\n",
    "               'precision':output_dict_resnet['precision_val'],\n",
    "               'sensitivity':output_dict_resnet['sensitivity_val'],\n",
    "               'specificity':output_dict_resnet['specificity_val'],\n",
    "              },'resnet34')\n",
    "\n",
    "\n",
    "cm_list = output_dict_resnet['confusion_matrix_test']\n",
    "for i,cm in enumerate(cm_list):\n",
    "    print(f'split {i}')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
